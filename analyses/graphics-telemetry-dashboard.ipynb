{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__To run individual analyses, run each block until you reach the big \"ANALYSES ARE BELOW\" marker. Then proceed to skip to whatever analysis you like.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import json, time, sys\n",
    "from __future__ import division\n",
    "from moztelemetry import get_pings, get_pings_properties, get_one_ping_per_client\n",
    "def fmt_date(d):\n",
    "    return d.strftime(\"%Y%m%d\")\n",
    "def repartition(pipeline):\n",
    "    return pipeline.repartition(MaxPartitions).cache()\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "MaxPartitions = sc.defaultParallelism * 4\n",
    "StartTime = datetime.datetime.now()\n",
    "\n",
    "# Configuration for general data that spans all Firefox versions. Roughly,\n",
    "# there are 100mil release channel pings, 7mil beta, 400k aurora, and 180k\n",
    "# nightly for the given time window.\n",
    "#\n",
    "# To not explode the analysis time, the fractions below are adjusted\n",
    "# around having ~6mil users total for all channels, or ~3mil total for\n",
    "# beta+.\n",
    "GeneralTimeWindow = 14\n",
    "ReleaseFraction = 0.05\n",
    "BetaFraction = 0.3\n",
    "NightlyAndAuroraFraction = 0.5\n",
    "\n",
    "# This folder is created by default when run as a Telemetry job, but\n",
    "# not through a temporary Spark cluster.\n",
    "!mkdir -p output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of keys for properties on session pings that we care about.\n",
    "MonitorsKey =          'environment/system/gfx/monitors'\n",
    "ArchKey =              'environment/build/architecture'\n",
    "FeaturesKey          = 'environment/system/gfx/features'\n",
    "UserPrefsKey         = 'environment/settings/userPrefs'\n",
    "DeviceResetReasonKey = 'payload/histograms/DEVICE_RESET_REASON'\n",
    "SANITY_TEST =          'payload/histograms/GRAPHICS_SANITY_TEST'\n",
    "SANITY_TEST_REASON =   'payload/histograms/GRAPHICS_SANITY_TEST_REASON'\n",
    "STARTUP_TEST_KEY =     'payload/histograms/GRAPHICS_DRIVER_STARTUP_TEST'\n",
    "WebGLSuccessKey =      'payload/histograms/CANVAS_WEBGL_SUCCESS'\n",
    "WebGL2SuccessKey =     'payload/histograms/CANVAS_WEBGL2_SUCCESS'\n",
    "PluginModelKey =       'payload/histograms/PLUGIN_DRAWING_MODEL'\n",
    "MediaDecoderKey =      'payload/histograms/MEDIA_DECODER_BACKEND_USED'\n",
    "LayersD3D9FailureKey = 'payload/keyedHistograms/D3D9_COMPOSITING_FAILURE_ID'\n",
    "LayersD3D11FailureKey ='payload/keyedHistograms/D3D11_COMPOSITING_FAILURE_ID'\n",
    "LayersOGLFailureKey =  'payload/keyedHistograms/OPENGL_COMPOSITING_FAILURE_ID'\n",
    "WebGLAcclFailureKey =  'payload/keyedHistograms/CANVAS_WEBGL_ACCL_FAILURE_ID'\n",
    "WebGLFailureKey =      'payload/keyedHistograms/CANVAS_WEBGL_FAILURE_ID'\n",
    "\n",
    "# This is the filter list, so we only select the above properties.\n",
    "PropertyList = [\n",
    "    FeaturesKey,\n",
    "    UserPrefsKey,\n",
    "    MonitorsKey,\n",
    "    ArchKey,\n",
    "    DeviceResetReasonKey,\n",
    "    SANITY_TEST,\n",
    "    SANITY_TEST_REASON,\n",
    "    STARTUP_TEST_KEY,\n",
    "    WebGLSuccessKey,\n",
    "    WebGL2SuccessKey,\n",
    "    PluginModelKey,\n",
    "    MediaDecoderKey,\n",
    "    LayersD3D9FailureKey,\n",
    "    LayersD3D11FailureKey,\n",
    "    LayersOGLFailureKey,\n",
    "    WebGLAcclFailureKey,\n",
    "    WebGLFailureKey,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Helper function block for fetching and filtering pings. #\n",
    "###########################################################\n",
    "def union_pipelines(a, b):\n",
    "    if a is None:\n",
    "        return b\n",
    "    return a + b\n",
    "\n",
    "# Helper for scaling back \n",
    "class FeatureDate(object):\n",
    "    def __init__(self, availDate, mergeDate):\n",
    "        self.availDate = availDate\n",
    "        self.mergeDate = mergeDate\n",
    "\n",
    "        \n",
    "    # If a feature has not been available for a given time window yet,\n",
    "    # scale up the amount of samples we take to get an equivalent sample\n",
    "    # size.\n",
    "    def scaleToAvailability(self, startDate, endDate, availDate, fraction):\n",
    "        assert availDate > startDate\n",
    "        want_days = (endDate - startDate).days\n",
    "        avail_days = (endDate - availDate).days\n",
    "        fraction = min((fraction * want_days) / avail_days, 1)\n",
    "        return availDate, fraction\n",
    "    \n",
    "    # Try to scale up/down a channel's ping amount based on how long a\n",
    "    # feature has been available.\n",
    "    def computeChannel(self, startDate, endDate, availDate, fraction):\n",
    "        if availDate <= startDate:\n",
    "            return {\n",
    "                'build_id': (startDate, endDate),\n",
    "                'fraction': fraction\n",
    "            }\n",
    "        \n",
    "        newStartDate, newFraction = self.scaleToAvailability(\n",
    "            startDate = startDate,\n",
    "            endDate = endDate,\n",
    "            availDate = availDate,\n",
    "            fraction = fraction)\n",
    "        return {\n",
    "            'build_id': (newStartDate, endDate),\n",
    "            'fraction': newFraction\n",
    "        }\n",
    "    \n",
    "    def computeChannels(self, startDate, endDate):\n",
    "        channels = {}\n",
    "        channels['nightly'] = self.computeChannel(\n",
    "            startDate = startDate,\n",
    "            endDate = endDate,\n",
    "            availDate = self.availDate,\n",
    "            fraction = NightlyAndAuroraFraction)\n",
    "        \n",
    "        # If we haven't merged to aurora, return now.\n",
    "        if (endDate - self.mergeDate).days < 1:\n",
    "            return channels\n",
    "        channels['aurora'] = self.computeChannel(\n",
    "            startDate = startDate,\n",
    "            endDate = endDate,\n",
    "            availDate = self.mergeDate,\n",
    "            fraction = NightlyAndAuroraFraction)\n",
    "        \n",
    "        # Guesstimate the merge date for Beta\n",
    "        betaDate = self.mergeDate + datetime.timedelta(6 * 7 + 1)\n",
    "        if (endDate - betaDate).days < 1:\n",
    "            return channels\n",
    "        channels['beta'] = self.computeChannel(\n",
    "            startDate = startDate,\n",
    "            endDate = endDate,\n",
    "            availDate = betaDate,\n",
    "            fraction = BetaFraction)\n",
    "        \n",
    "        # If we've probably released this feature, just select across all\n",
    "        # sessions.\n",
    "        relDate = betaDate + datetime.timedelta(6 * 7 + 1)\n",
    "        if (endDate - relDate).days > 1:\n",
    "            return None\n",
    "\n",
    "        return channels\n",
    "    \n",
    "def BuildPingArgs(**kwargs):\n",
    "    timeWindow = kwargs.pop('timeWindow', GeneralTimeWindow)\n",
    "    channel = kwargs.pop('channel', None)\n",
    "    feature = kwargs.pop('feature', None)\n",
    "    \n",
    "    # Since builds take a bit to disseminate, go back about 4 hour. This is a\n",
    "    # completely made up number.\n",
    "    limit = datetime.timedelta(0, 60 * 60 * 4)\n",
    "    now = datetime.datetime.now()\n",
    "    start = now - datetime.timedelta(timeWindow) - limit\n",
    "    end = now - limit\n",
    "        \n",
    "    # Build channel properties.\n",
    "    channels = None\n",
    "    if channel is None and feature is not None:\n",
    "        channels = feature.computeChannels(start, end)\n",
    "    elif channel is not None:\n",
    "        # Use the old algorithm of build_id.\n",
    "        channels = {\n",
    "            channel: {\n",
    "                'range': (start, end),\n",
    "                'fraction': kwargs.pop('fraction'),\n",
    "                'build_id': (start, end),\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # If we don't have any channel info, we select across all pings. Here we\n",
    "    # use the submission date rather than build_id, since most pings will be\n",
    "    # from the release channel where builds are infrequent.\n",
    "    if channels is None:\n",
    "        channels = {\n",
    "            '*': {\n",
    "                'submission_date': (start, end),\n",
    "                'fraction': kwargs.pop('fraction', ReleaseFraction)\n",
    "            }\n",
    "        }\n",
    "\n",
    "    baseArgs = {\n",
    "        'app': 'Firefox',\n",
    "        'schema': 'v4',\n",
    "    }\n",
    "    \n",
    "    def fmt_dates(tup):\n",
    "        return (fmt_date(tup[0]), fmt_date(tup[1]))\n",
    "    def day_count(tup):\n",
    "        return (tup[1] - tup[0]).days\n",
    "\n",
    "    metadata = []\n",
    "    for channel in channels:\n",
    "        info = {}\n",
    "        new_args = baseArgs.copy()\n",
    "        \n",
    "        if channel != '*':\n",
    "            new_args['channel'] = channel\n",
    "        info['channel'] = channel\n",
    "        \n",
    "        old_args = channels[channel]\n",
    "        \n",
    "        new_args['fraction'] = old_args['fraction']\n",
    "        info['fraction'] = old_args['fraction']\n",
    "            \n",
    "        if 'build_id' in old_args:\n",
    "            new_args['build_id'] = fmt_dates(old_args['build_id'])\n",
    "            info['build_range'] = day_count(old_args['build_id'])\n",
    "        if 'submission_date' in old_args:\n",
    "            new_args['submission_date'] = fmt_dates(old_args['submission_date'])\n",
    "            info['day_range'] = day_count(old_args['submission_date'])\n",
    "        \n",
    "        metadata.append({\n",
    "            'args': new_args,\n",
    "            'info': info,\n",
    "        })  \n",
    "    return metadata\n",
    "\n",
    "def FetchRawPings(**kwargs):\n",
    "    timestamp = datetime.datetime.now()\n",
    "    metadata = BuildPingArgs(**kwargs)\n",
    "    pings = None\n",
    "    for props in metadata:\n",
    "        subset = get_pings(sc, **props['args'])\n",
    "        pings = union_pipelines(pings, subset)\n",
    "    return pings, {\n",
    "        'metadata': metadata,\n",
    "        'timestamp': timestamp,\n",
    "    }\n",
    "        \n",
    "# Transform each ping to make it easier to work with in later stages.\n",
    "def Validate(p):\n",
    "    name = p.get(\"environment/system/os/name\") or 'w'\n",
    "    version = p.get(\"environment/system/os/version\") or '0'\n",
    "    if name == 'Linux':\n",
    "        p['OSVersion'] = None\n",
    "        p['OS'] = 'Linux'\n",
    "        p['OSName'] = 'Linux'\n",
    "    elif name == 'Windows_NT':\n",
    "        spmaj = p.get(\"environment/system/os/servicePackMajor\") or '0'\n",
    "        p['OSVersion'] = version + '.' + str(spmaj)\n",
    "        p['OS'] = 'Windows-' + version + '.' + str(spmaj)\n",
    "        p['OSName'] = 'Windows'\n",
    "    elif name == 'Darwin':\n",
    "        p['OSVersion'] = version\n",
    "        p['OS'] = 'Darwin-' + version\n",
    "        p['OSName'] = 'Darwin'\n",
    "    else:\n",
    "        p['OSVersion'] = version\n",
    "        p['OS'] = '{0}-{1}'.format(name, version)\n",
    "        p['OSName'] = name\n",
    "    \n",
    "    # Telemetry data isn't guaranteed to be well-formed so unfortunately\n",
    "    # we have to do some validation on it. If we get to the end, we set\n",
    "    # p['valid'] to True, and this gets filtered over later. In addition\n",
    "    # we have a wrapper below to help fetch strings that may be null.\n",
    "    if not p.get(\"environment/build/version\", None):\n",
    "        return p\n",
    "    p['FxVersion'] = p[\"environment/build/version\"].split('.')[0]\n",
    "    \n",
    "    # Verify that we have at least one adapter.\n",
    "    try:\n",
    "        adapter = p[\"environment/system/gfx/adapters\"][0]\n",
    "    except:\n",
    "        return p\n",
    "    if adapter is None or not hasattr(adapter, '__getitem__'):\n",
    "        return p\n",
    "    \n",
    "    def T(obj, key):\n",
    "        return obj.get(key, None) or 'Unknown'\n",
    "    \n",
    "    # We store the device ID as a vendor/device string, because the device ID\n",
    "    # alone is not enough to determine whether the key is unique.\n",
    "    #\n",
    "    # We also merge 'Intel Open Source Technology Center' with the device ID\n",
    "    # that should be reported, 0x8086, for simplicity.\n",
    "    vendorID = T(adapter, 'vendorID')\n",
    "    if vendorID == u'Intel Open Source Technology Center':\n",
    "        p['vendorID'] = u'0x8086'\n",
    "    else:\n",
    "        p['vendorID'] = vendorID\n",
    "    p['deviceID'] = u'{0}/{1}'.format(p['vendorID'], T(adapter, 'deviceID'))\n",
    "    p['driverVersion'] = u'{0}/{1}'.format(p['vendorID'], T(adapter, 'driverVersion'))\n",
    "    p['deviceAndDriver'] = u'{0}/{1}'.format(p['deviceID'], T(adapter, 'driverVersion'))\n",
    "        \n",
    "    p['valid'] = True\n",
    "    return p\n",
    "\n",
    "def reduce_pings(pings):\n",
    "    return get_pings_properties(pings, [\n",
    "      'clientId',\n",
    "      \"creationDate\",\n",
    "      \"environment/build/version\",\n",
    "      \"environment/build/buildId\",\n",
    "      \"environment/system/memoryMB\",\n",
    "      \"environment/system/isWow64\",\n",
    "      \"environment/system/cpu\",\n",
    "      \"environment/system/os/name\",\n",
    "      \"environment/system/os/version\",\n",
    "      \"environment/system/os/servicePackMajor\",\n",
    "      \"environment/system/gfx/adapters\",\n",
    "      \"payload/info/revision\",\n",
    "    ] + PropertyList)\n",
    "\n",
    "def FormatPings(pings):\n",
    "    pings = reduce_pings(pings)\n",
    "    pings = get_one_ping_per_client(pings)\n",
    "    pings = pings.map(Validate)\n",
    "    filtered_pings = pings.filter(lambda p: p.get('valid', False) == True)\n",
    "    return filtered_pings.cache()\n",
    "\n",
    "def FetchAndFormat(**kwargs):\n",
    "    raw_pings, info = FetchRawPings(**kwargs)\n",
    "    return FormatPings(raw_pings), info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# Helper function block for massaging pings into aggregate data. #\n",
    "##################################################################\n",
    "\n",
    "# Take each key in |b| and add it to |a|, accumulating its value into\n",
    "# |a| if it already exists.\n",
    "def combiner(a, b):\n",
    "    result = a\n",
    "    for key in b:\n",
    "        countA = a.get(key, 0)\n",
    "        countB = b[key]\n",
    "        result[key] = countA + countB\n",
    "    return result\n",
    "\n",
    "# Helper for reduceByKey => count.\n",
    "def map_x_to_count(data, sourceKey):\n",
    "    def extract(p):\n",
    "        return (p[sourceKey],)\n",
    "    return data.map(extract).countByKey()\n",
    "\n",
    "# After reduceByKey(combiner), we get a mapping like:\n",
    "#  key => { variable => value }\n",
    "#\n",
    "# This function collapses 'variable' instances below a threshold into\n",
    "# a catch-all identifier ('Other').\n",
    "def coalesce_to_n_items(agg, max_items):\n",
    "    obj = []\n",
    "    for superkey, breakdown in agg:\n",
    "        if len(breakdown) <= max_items:\n",
    "            obj += [(superkey, breakdown)]\n",
    "            continue\n",
    "        items = sorted(breakdown.items(), key=lambda obj: obj[1], reverse=True)\n",
    "        new_breakdown = {k: v for k, v in items[0:max_items]}\n",
    "        total = 0\n",
    "        for k, v in items[max_items:]:\n",
    "            total += v\n",
    "        if total:\n",
    "            new_breakdown['Other'] = new_breakdown.get('Other', 0) + total\n",
    "        obj += [(superkey, new_breakdown)]\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "# Helper for writing files. #\n",
    "#############################\n",
    "\n",
    "def ApplyPingInfo(obj, **kwargs):\n",
    "    if 'pings' not in kwargs:\n",
    "        return\n",
    "    \n",
    "    pings, info = kwargs.pop('pings')\n",
    "    \n",
    "    # To make the sample source information more transparent, we include\n",
    "    # the breakdown of Firefox channel numbers.\n",
    "    if '__share' not in info:\n",
    "        info['__share'] = map_x_to_count(pings, 'FxVersion')\n",
    "    \n",
    "    obj['sessions'] = {\n",
    "        'count': pings.count(),\n",
    "        'timestamp': time.mktime(info['timestamp'].timetuple()),\n",
    "        'shortdate': fmt_date(info['timestamp']),\n",
    "        'metadata': info['metadata'],\n",
    "        'share': info['__share'],\n",
    "    }\n",
    "    \n",
    "def Export(filename, obj, **kwargs):\n",
    "    with open('output/{0}.json'.format(filename), 'w') as fp:\n",
    "        json.dump(obj, fp)\n",
    "    if kwargs.pop('save_history', True):\n",
    "        stamp = obj['sessions']['shortdate']\n",
    "        with open('output/{0}-{1}.json'.format(filename, stamp), 'w') as fp:\n",
    "            json.dump(obj, fp)\n",
    "        \n",
    "def TimedExport(filename, callback, **kwargs):\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    obj = callback()\n",
    "    ApplyPingInfo(obj, **kwargs)\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    elapsed = end - start\n",
    "    obj['phaseTime'] = elapsed.total_seconds()\n",
    "    \n",
    "    Export(filename, obj, **kwargs)\n",
    "    export_time = datetime.datetime.now() - end\n",
    "    \n",
    "    print('Computed {0} in {1} seconds.'.format(filename, elapsed.total_seconds()))\n",
    "    print('Exported {0} in {1} seconds.'.format(filename, export_time.total_seconds()))\n",
    "    \n",
    "# Profiler for debugging.\n",
    "class Prof(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def __enter__(self):\n",
    "        self.sout('Starting {0}... '.format(self.name))\n",
    "        self.start = datetime.datetime.now()\n",
    "        return None\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.end = datetime.datetime.now()\n",
    "        self.sout('... {0}: {1}s'.format(self.name, (self.end - self.start).total_seconds()))\n",
    "    def sout(self, s):\n",
    "        sys.stdout.write(s)\n",
    "        sys.stdout.write('\\n')\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quiet_logs(sc):\n",
    "  logger = sc._jvm.org.apache.log4j\n",
    "  logger.LogManager.getLogger(\"org\").setLevel(logger.Level.ERROR)\n",
    "  logger.LogManager.getLogger(\"akka\").setLevel(logger.Level.ERROR)\n",
    "quiet_logs(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get a general ping sample across all Firefox channels.\n",
    "with Prof(\"General pings\") as px:\n",
    "    GeneralPings, GeneralPingInfo  = FetchAndFormat()\n",
    "    GeneralPings = GeneralPings.cache()\n",
    "\n",
    "    # Windows gets some preferential breakdown treatment.\n",
    "    WindowsPings = GeneralPings.filter(lambda p: p['OSName'] == 'Windows')\n",
    "    WindowsPings = WindowsPings.cache()\n",
    "    \n",
    "    MacPings = GeneralPings.filter(lambda p: p['OSName'] == 'Darwin')\n",
    "    MacPings = repartition(MacPings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ANALYSES ARE BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Results by operating system.\n",
    "if '__share' not in GeneralPingInfo:\n",
    "    GeneralPingInfo['__share'] = map_x_to_count(GeneralPings, 'FxVersion')\n",
    "\n",
    "def GetGeneralStatisticsForSubset(subset, windows_subset):\n",
    "    OSShare = map_x_to_count(subset, 'OSName')\n",
    "\n",
    "    # Results by Windows version.\n",
    "    WindowsShare = map_x_to_count(windows_subset, 'OSVersion')\n",
    "\n",
    "    # Top-level stats.\n",
    "    VendorShare = map_x_to_count(subset, 'vendorID')\n",
    "    \n",
    "    return {\n",
    "        'os': OSShare,\n",
    "        'windows': WindowsShare,\n",
    "        'vendors': VendorShare,\n",
    "    }\n",
    "\n",
    "def GetGeneralStatistics():\n",
    "    obj = {}\n",
    "    obj['devices'] = map_x_to_count(GeneralPings, 'deviceID')\n",
    "    obj['drivers'] = map_x_to_count(GeneralPings, 'driverVersion')\n",
    "    \n",
    "    byFx = {}\n",
    "    with Prof('general stats for all') as px:\n",
    "        byFx['all'] = GetGeneralStatisticsForSubset(GeneralPings, WindowsPings)\n",
    "    \n",
    "    for key in GeneralPingInfo['__share']:\n",
    "        subset = GeneralPings.filter(lambda p: p['FxVersion'] == key)\n",
    "        windows = subset.filter(lambda p: p['OSName'] == 'Windows')\n",
    "        subset = repartition(subset)\n",
    "        windows = repartition(windows)\n",
    "        with Prof('general stats for ' + key) as px:\n",
    "            byFx[key] = GetGeneralStatisticsForSubset(subset, windows)\n",
    "        \n",
    "    obj['byFx'] = byFx\n",
    "    return obj\n",
    "        \n",
    "TimedExport(filename = 'general-statistics',\n",
    "            callback = GetGeneralStatistics,\n",
    "            pings = (GeneralPings, GeneralPingInfo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device/driver search database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetDriverStatistics():\n",
    "    obj = {}\n",
    "    obj['deviceAndDriver'] = map_x_to_count(GeneralPings,'deviceAndDriver')\n",
    "    return obj\n",
    "\n",
    "TimedExport(filename = 'device-statistics',\n",
    "            callback = GetDriverStatistics,\n",
    "            save_history = False, # No demand yet, and too much data.\n",
    "            pings = (GeneralPings, GeneralPingInfo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDR Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "# Perform the TDR analysis. #\n",
    "#############################\n",
    "def GetTDRStatistics():\n",
    "    NumTDRReasons = 8\n",
    "    def ping_has_tdr_for(p, reason):\n",
    "        return p[DeviceResetReasonKey][reason] > 0\n",
    "\n",
    "    # Specialized version of map_x_to_y, for TDRs. We cast to int because for\n",
    "    # some reason the values Spark returns do not serialize with JSON.\n",
    "    def map_reason_to_vendor(p, reason, destKey):\n",
    "        return (int(reason), { p[destKey]: int(p[DeviceResetReasonKey][reason]) })\n",
    "    def map_vendor_to_reason(p, reason, destKey):\n",
    "        return (p[destKey], { int(reason): int(p[DeviceResetReasonKey][reason]) })\n",
    "\n",
    "    # Filter out pings that do not have any TDR data. We expect this to be a huge reduction\n",
    "    # in the sample set, and the resulting partition count gets way off. We repartition\n",
    "    # immediately for performance.\n",
    "    TDRSubset = WindowsPings.filter(lambda p: p.get(DeviceResetReasonKey, None) is not None)\n",
    "    TDRSubset = TDRSubset.repartition(MaxPartitions)\n",
    "    TDRSubset = TDRSubset.cache()\n",
    "\n",
    "    # Aggregate the device reset data.\n",
    "    TDRResults = TDRSubset.map(lambda p: p[DeviceResetReasonKey]).reduce(lambda x, y: x + y)\n",
    "\n",
    "    # For each TDR reason, get a list tuple of (reason, vendor => resetCount). Then\n",
    "    # we combine these into a single series.\n",
    "    reason_to_vendor_tuples = None\n",
    "    vendor_to_reason_tuples = None\n",
    "    for reason in xrange(1, NumTDRReasons):\n",
    "        subset = TDRSubset.filter(lambda p: ping_has_tdr_for(p, reason))\n",
    "        subset = subset.cache()\n",
    "\n",
    "        tuples = subset.map(lambda p: map_reason_to_vendor(p, reason, 'vendorID'))\n",
    "        reason_to_vendor_tuples = union_pipelines(reason_to_vendor_tuples, tuples)\n",
    "\n",
    "        tuples = subset.map(lambda p: map_vendor_to_reason(p, reason, 'vendorID'))\n",
    "        vendor_to_reason_tuples = union_pipelines(vendor_to_reason_tuples, tuples)\n",
    "\n",
    "    TDRReasonToVendor = reason_to_vendor_tuples.reduceByKey(combiner, MaxPartitions)\n",
    "    TDRVendorToReason = vendor_to_reason_tuples.reduceByKey(combiner, MaxPartitions)\n",
    "    \n",
    "    return {\n",
    "        'tdrPings': TDRSubset.count(),\n",
    "        'results': [int(value) for value in TDRResults],\n",
    "        'reasonToVendor': TDRReasonToVendor.collect(),\n",
    "        'vendorToReason': TDRVendorToReason.collect(),\n",
    "    }\n",
    "    \n",
    "# Write TDR statistics.\n",
    "TimedExport(filename = 'tdr-statistics',\n",
    "            callback = GetTDRStatistics,\n",
    "            pings = (WindowsPings, GeneralPingInfo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Get system statistics. #\n",
    "##########################\n",
    "CpuKey = 'environment/system/cpu'\n",
    "MemoryKey = 'environment/system/memoryMB'\n",
    "\n",
    "def GetBucketedMemory(pings): \n",
    "    def get_bucket(p):\n",
    "        x = int(p / 1000)\n",
    "        if x < 1:\n",
    "            return 'less_1gb'\n",
    "        if x <= 4:\n",
    "            return x\n",
    "        if x <= 8:\n",
    "            return '4_to_8'\n",
    "        if x <= 16:\n",
    "            return '8_to_16'\n",
    "        if x <= 32:\n",
    "            return '16_to_32'\n",
    "        return 'more_32'\n",
    "    \n",
    "    memory_rdd = pings.map(lambda p: p.get(MemoryKey, 0))\n",
    "    memory_rdd = memory_rdd.filter(lambda p: p > 0)\n",
    "    memory_rdd = memory_rdd.map(lambda p: (get_bucket(p),))\n",
    "    memory_rdd = repartition(memory_rdd)\n",
    "    return memory_rdd.countByKey()\n",
    "\n",
    "def GetCpuFeatures(pings):\n",
    "    cpuid_rdd = pings.map(lambda p: p.get(CpuKey, None))\n",
    "    cpuid_rdd = cpuid_rdd.filter(lambda p: p is not None)\n",
    "    cpuid_rdd = cpuid_rdd.map(lambda p: p.get('extensions', None))\n",
    "    \n",
    "    # Unfortunately, Firefox 39 had a bug where CPU features could be reported even\n",
    "    # if they weren't present. To detect this we filter pings that have ARMv6 support\n",
    "    # on x86/64.\n",
    "    cpuid_rdd = cpuid_rdd.filter(lambda p: p is not None and 'hasARMv6' not in p)\n",
    "    cpuid_rdd = repartition(cpuid_rdd)\n",
    "\n",
    "    # Count before we blow up the list.\n",
    "    with Prof('cpu count for x86') as px:\n",
    "        total = cpuid_rdd.count()\n",
    "\n",
    "    cpuid_rdd = cpuid_rdd.flatMap(lambda p: [(ex, 1) for ex in p])\n",
    "    with Prof('cpu features for x86') as px:\n",
    "        feature_map = cpuid_rdd.countByKey()\n",
    "        \n",
    "    return {\n",
    "        'total': total,\n",
    "        'features': feature_map,\n",
    "    }\n",
    "\n",
    "def GetSystemStatistics():\n",
    "    def get_logical_cores(p):\n",
    "        cpu = p.get(CpuKey, None)\n",
    "        if cpu is None:\n",
    "            return 'unknown'\n",
    "        return cpu.get('count', 'unknown')\n",
    "    with Prof('logical cores') as px:\n",
    "        logical_cores = GeneralPings.map(lambda p: (get_logical_cores(p),)).countByKey()\n",
    "    \n",
    "    cpu_features = GetCpuFeatures(GeneralPings)\n",
    "    \n",
    "    with Prof('memory buckets') as px:\n",
    "        memory = GetBucketedMemory(GeneralPings)\n",
    "        \n",
    "    def get_os_bits(p):\n",
    "        arch = p.get(ArchKey, 'unknown')\n",
    "        if arch == 'x86-64':\n",
    "            return '64'\n",
    "        if arch == 'x86':\n",
    "            wow64 = p.get(\"environment/system/isWow64\", False)\n",
    "            if wow64:\n",
    "                return '32_on_64'\n",
    "            return '32'\n",
    "        return 'unknown'\n",
    "    \n",
    "    with Prof('OS bit count') as px:\n",
    "        os_bits = WindowsPings.map(lambda p: (get_os_bits(p),)).countByKey()\n",
    "    \n",
    "    return {\n",
    "        'logical_cores': logical_cores,\n",
    "        'x86': cpu_features,\n",
    "        'memory': memory,\n",
    "        'wow': os_bits,\n",
    "    }\n",
    "\n",
    "TimedExport(filename = 'system-statistics',\n",
    "            callback = GetSystemStatistics,\n",
    "            pings = (GeneralPings, GeneralPingInfo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Test Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up constants.\n",
    "SANITY_TEST_PASSED = 0\n",
    "SANITY_TEST_FAILED_RENDER = 1\n",
    "SANITY_TEST_FAILED_VIDEO = 2\n",
    "SANITY_TEST_CRASHED = 3\n",
    "SANITY_TEST_TIMEDOUT = 4\n",
    "SANITY_TEST_LAST_VALUE = 5\n",
    "SANITY_TEST_REASON_FIRST_RUN = 0\n",
    "SANITY_TEST_REASON_FIREFOX_CHANGED = 1\n",
    "SANITY_TEST_REASON_DEVICE_CHANGED = 2\n",
    "SANITY_TEST_REASON_DRIVER_CHANGED = 3\n",
    "SANITY_TEST_REASON_LAST_VALUE = 4\n",
    "\n",
    "# Filter sanity test pings.\n",
    "def ping_has_sanity_test(p):\n",
    "    return p.get(SANITY_TEST, None) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# Sanity test analysis. #\n",
    "#########################\n",
    "def GetSanityTestsForSlice(filter_func, is_xp = False):\n",
    "    sanity_test_pings = WindowsPings.filter(filter_func)\n",
    "    data = sanity_test_pings.filter(ping_has_sanity_test)\n",
    "    \n",
    "    # We don't want to fold FAILED_LAYERS and FAILED_VIDEO into the same\n",
    "    # resultset, so we use this function to split them out.\n",
    "    def get_sanity_test_result(p):\n",
    "        if p[SANITY_TEST][SANITY_TEST_PASSED] > 0:\n",
    "            return SANITY_TEST_PASSED\n",
    "        if p[SANITY_TEST][SANITY_TEST_CRASHED] > 0:\n",
    "            return SANITY_TEST_CRASHED\n",
    "        if p[SANITY_TEST][SANITY_TEST_FAILED_RENDER] > 0:\n",
    "            return SANITY_TEST_FAILED_RENDER\n",
    "        if p[SANITY_TEST][SANITY_TEST_FAILED_VIDEO] > 0:\n",
    "            return SANITY_TEST_FAILED_VIDEO\n",
    "        return SANITY_TEST_LAST_VALUE\n",
    "    \n",
    "    # Aggregate the sanity test data.\n",
    "    with Prof('initial map') as px:\n",
    "        SanityTestResults = data.map(lambda p: (get_sanity_test_result(p),)).countByKey()\n",
    "\n",
    "    with Prof('share resolve') as px:\n",
    "        os_share = map_x_to_count(data, 'OSVersion')\n",
    "        \n",
    "    with Prof('ping_count') as px:\n",
    "        sanity_test_count = data.count()\n",
    "        ping_count = sanity_test_pings.count()\n",
    "\n",
    "    sanity_test_by_vendor = None\n",
    "    sanity_test_by_os = None\n",
    "    sanity_test_by_device = None\n",
    "    sanity_test_by_driver = None\n",
    "    for value in xrange(SANITY_TEST_FAILED_RENDER, SANITY_TEST_LAST_VALUE):\n",
    "        if is_xp and value == SANITY_TEST_FAILED_VIDEO:\n",
    "            continue\n",
    "\n",
    "        subset = data.filter(lambda p: get_sanity_test_result(p) == value)\n",
    "\n",
    "        tuples = subset.map(lambda p: (value, { p['vendorID']: int(p[SANITY_TEST][value]) }))\n",
    "        sanity_test_by_vendor = union_pipelines(sanity_test_by_vendor, tuples)\n",
    "    \n",
    "        tuples = subset.map(lambda p: (value, { p['OS']: int(p[SANITY_TEST][value]) }))\n",
    "        sanity_test_by_os = union_pipelines(sanity_test_by_os, tuples)\n",
    "    \n",
    "        tuples = subset.map(lambda p: (value, { p['deviceID']: int(p[SANITY_TEST][value]) }))\n",
    "        sanity_test_by_device = union_pipelines(sanity_test_by_device, tuples)\n",
    "    \n",
    "        tuples = subset.map(lambda p: (value, { p['driverVersion']: int(p[SANITY_TEST][value]) }))\n",
    "        sanity_test_by_driver = union_pipelines(sanity_test_by_driver, tuples)\n",
    "            \n",
    "    sanity_test_by_vendor = repartition(sanity_test_by_vendor)\n",
    "    sanity_test_by_os = repartition(sanity_test_by_os)\n",
    "    sanity_test_by_device = repartition(sanity_test_by_device)\n",
    "    sanity_test_by_driver = repartition(sanity_test_by_driver)\n",
    "    \n",
    "    with Prof('vendor resolve') as px:\n",
    "        SanityTestByVendor = sanity_test_by_vendor.reduceByKey(combiner)\n",
    "    with Prof('os resolve') as px:\n",
    "        SanityTestByOS = sanity_test_by_os.reduceByKey(combiner)\n",
    "    with Prof('device resolve') as px:\n",
    "        SanityTestByDevice = sanity_test_by_device.reduceByKey(combiner)\n",
    "    with Prof('driver resolve') as px:\n",
    "        SanityTestByDriver = sanity_test_by_driver.reduceByKey(combiner)\n",
    "        \n",
    "    print('Partitions: {0},{1},{2},{3}'.format(\n",
    "        SanityTestByVendor.getNumPartitions(),\n",
    "        SanityTestByOS.getNumPartitions(),\n",
    "        SanityTestByDevice.getNumPartitions(),\n",
    "        SanityTestByDriver.getNumPartitions()))\n",
    "        \n",
    "    with Prof('vendor collect') as px:\n",
    "        byVendor = SanityTestByVendor.collect()\n",
    "    with Prof('os collect') as px:\n",
    "        byOS = SanityTestByOS.collect()\n",
    "    with Prof('device collect') as px:\n",
    "        byDevice = SanityTestByDevice.collect()\n",
    "    with Prof('driver collect') as px:\n",
    "        byDriver = SanityTestByDriver.collect()\n",
    "    \n",
    "    return {\n",
    "        'sanityTestPings': sanity_test_count,\n",
    "        'totalPings': ping_count,\n",
    "        'results': SanityTestResults,\n",
    "        'byVendor': byVendor,\n",
    "        'byOS': byOS,\n",
    "        'byDevice': coalesce_to_n_items(byDevice, 10),\n",
    "        'byDriver': coalesce_to_n_items(byDriver, 10),\n",
    "        'windows': os_share,\n",
    "    }\n",
    "\n",
    "def GetSanityTests(): \n",
    "    obj = {}\n",
    "    \n",
    "    def windows_xp(p):\n",
    "        return p['OSName'] == 'Windows' and p['OSVersion'].startswith('5.')\n",
    "    obj['windowsXP'] = GetSanityTestsForSlice(windows_xp, is_xp = True)\n",
    "    \n",
    "    def windows_vista(p):\n",
    "        return p['OSName'] == 'Windows' and not p['OSVersion'].startswith('5.')\n",
    "    obj['windows'] = GetSanityTestsForSlice(windows_vista)\n",
    "    \n",
    "    return obj\n",
    "    \n",
    "# Write Sanity Test statistics.\n",
    "TimedExport(filename = 'sanity-test-statistics',\n",
    "            callback = GetSanityTests,\n",
    "            pings = (WindowsPings, GeneralPingInfo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startup Crash Guard Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STARTUP_OK = 0\n",
    "STARTUP_ENV_CHANGED = 1\n",
    "STARTUP_CRASHED = 2\n",
    "STARTUP_ACCEL_DISABLED = 3\n",
    "    \n",
    "def GetStartupTests():\n",
    "    startup_test_pings = GeneralPings.filter(lambda p: p.get(STARTUP_TEST_KEY, None) is not None)\n",
    "    startup_test_pings = startup_test_pings.repartition(MaxPartitions)\n",
    "    startup_test_pings = startup_test_pings.cache()\n",
    "\n",
    "    StartupTestResults = startup_test_pings.map(lambda p: p[STARTUP_TEST_KEY]).reduce(lambda x, y: x + y)\n",
    "    \n",
    "    os_share = map_x_to_count(startup_test_pings, 'OS')\n",
    "    \n",
    "    return {\n",
    "        'startupTestPings': startup_test_pings.count(),\n",
    "        'results': [int(i) for i in StartupTestResults],\n",
    "        'windows': os_share,\n",
    "    }\n",
    "    \n",
    "# Write startup test results.\n",
    "TimedExport(filename = 'startup-test-statistics',\n",
    "            callback = GetStartupTests,\n",
    "            pings = (GeneralPings, GeneralPingInfo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_monitor_count(p):\n",
    "    monitors = p.get(MonitorsKey, None)\n",
    "    try:\n",
    "        return len(monitors)\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def get_monitor_res(p, i):\n",
    "    width = p[MonitorsKey][i].get('screenWidth', 0)\n",
    "    height = p[MonitorsKey][i].get('screenHeight', 0)\n",
    "    if width == 0 or height == 0:\n",
    "        return 'Unknown'\n",
    "    return '{0}x{1}'.format(width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetMonitorStatistics():\n",
    "    def get_monitor_rdds_for_index(data, i):\n",
    "        def get_refresh_rate(p):\n",
    "            refreshRate = p[MonitorsKey][i].get('refreshRate', 0)\n",
    "            return refreshRate if refreshRate > 1 else 'Unknown'\n",
    "        def get_resolution(p):\n",
    "            return get_monitor_res(p, i)\n",
    "    \n",
    "        monitors_at_index = data.filter(lambda p: get_monitor_count(p) == monitor_count)\n",
    "        monitors_at_index = repartition(monitors_at_index)\n",
    "        refresh_rates = monitors_at_index.map(lambda p: (get_refresh_rate(p),))\n",
    "        resolutions = monitors_at_index.map(lambda p: (get_resolution(p),))\n",
    "        return refresh_rates, resolutions\n",
    "    \n",
    "    MonitorCounts = WindowsPings.map(lambda p: (get_monitor_count(p),)).countByKey()\n",
    "    MonitorCounts.pop(0, None)\n",
    "\n",
    "    refresh_rates = None\n",
    "    resolutions = None\n",
    "    for monitor_count in MonitorCounts:\n",
    "        rate_subset, res_subset = get_monitor_rdds_for_index(WindowsPings, monitor_count - 1)\n",
    "        refresh_rates = union_pipelines(refresh_rates, rate_subset)\n",
    "        resolutions = union_pipelines(resolutions, res_subset)\n",
    "    \n",
    "    MonitorRefreshRates = refresh_rates.countByKey()\n",
    "    MonitorResolutions = resolutions.countByKey()\n",
    "    \n",
    "    return {\n",
    "        'counts': MonitorCounts,\n",
    "        'refreshRates': MonitorRefreshRates,\n",
    "        'resolutions': MonitorResolutions,\n",
    "    }\n",
    "\n",
    "TimedExport(filename = 'monitor-statistics',\n",
    "            callback = GetMonitorStatistics,\n",
    "            pings = (WindowsPings, GeneralPingInfo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mac OS X Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MacPings = GeneralPings.filter(lambda p: p['OSName'] == 'Darwin')\n",
    "MacPings = repartition(MacPings)\n",
    "      \n",
    "def GetMacStatistics():\n",
    "    version_map = map_x_to_count(MacPings, 'OSVersion')\n",
    "    \n",
    "    def get_scale(p):\n",
    "        monitors = p.get(MonitorsKey, None)\n",
    "        if not monitors:\n",
    "            return 'unknown'\n",
    "        try:\n",
    "            return monitors[0]['scale']\n",
    "        except:\n",
    "            'unknown'\n",
    "    scale_map = MacPings.map(lambda p: (get_scale(p),)).countByKey()\n",
    "    \n",
    "    def get_arch(p):\n",
    "        arch = p.get(ArchKey, 'unknown')\n",
    "        if arch == 'x86-64':\n",
    "            return '64'\n",
    "        if arch == 'x86':\n",
    "            return '32'\n",
    "        return 'unknown'\n",
    "    arch_map = MacPings.map(lambda p: (get_arch(p),)).countByKey()\n",
    "    \n",
    "    return {\n",
    "        'versions': version_map,\n",
    "        'retina': scale_map,\n",
    "        'arch': arch_map,\n",
    "    }\n",
    "\n",
    "TimedExport(filename = 'mac-statistics',\n",
    "            callback = GetMacStatistics,\n",
    "            pings = (MacPings, GeneralPingInfo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers for Compositor/Acceleration fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build graphics feature statistics.\n",
    "def get_compositor(p):\n",
    "    compositor = p[FeaturesKey].get('compositor', 'none')\n",
    "    if compositor == 'none':\n",
    "        userPrefs = p.get(UserPrefsKey, None)\n",
    "        if userPrefs is not None:\n",
    "            omtc = userPrefs.get('layers.offmainthreadcomposition.enabled', True)\n",
    "            if omtc != True:\n",
    "                return 'disabled'\n",
    "    return compositor\n",
    "\n",
    "def get_d3d11_status(p):\n",
    "    d3d11 = p[FeaturesKey].get('d3d11', None)\n",
    "    if not hasattr(d3d11, '__getitem__'):\n",
    "        return 'unknown'\n",
    "    if p['OSVersion'].startswith('5.1'):\n",
    "        # Windows XP is reported as blacklisted, which is deceptive. We rewrite\n",
    "        # it here so that it's clear why D3D11 is not available.\n",
    "        return 'unavailable'\n",
    "    status = d3d11.get('status', 'unknown')\n",
    "    if status != 'available':\n",
    "        return status\n",
    "    if d3d11.get('warp', False) == True:\n",
    "        return 'warp'\n",
    "    return d3d11.get('version', 'unknown')\n",
    "\n",
    "def get_warp_status(p):\n",
    "    if 'blacklisted' not in p[FeaturesKey]['d3d11']:\n",
    "        return 'unknown'\n",
    "    if p[FeaturesKey]['d3d11']['blacklisted'] == True:\n",
    "        return 'blacklist'\n",
    "    return 'device failure'\n",
    "\n",
    "def get_d2d_status(p):\n",
    "    d2d = p[FeaturesKey].get('d2d', None)\n",
    "    if not hasattr(d2d, '__getitem__'):\n",
    "        return ('unknown',)\n",
    "    status = d2d.get('status', 'unknown')\n",
    "    if status != 'available':\n",
    "        return (status,)\n",
    "    return (d2d.get('version', 'unknown'),)\n",
    "\n",
    "def has_working_d3d11(p):\n",
    "    d3d11 = p[FeaturesKey].get('d3d11', None)\n",
    "    if d3d11 is None:\n",
    "        return False\n",
    "    return d3d11.get('status') == 'available'\n",
    "\n",
    "def get_texture_sharing_status(p):\n",
    "    return (p[FeaturesKey]['d3d11'].get('textureSharing', 'unknown'),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows Compositor and Blacklisting Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get pings with graphics features. This landed in roughly the 7-19-2015 nightly.\n",
    "def windows_feature_filter(p):\n",
    "    return p['OSName'] == 'Windows' and p.get(FeaturesKey) is not None\n",
    "\n",
    "WindowsFeatures = WindowsPings.filter(lambda p: p.get(FeaturesKey) is not None)\n",
    "WindowsFeatures = WindowsFeatures.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We skip certain windows versions in detail lists since this phase is\n",
    "# very expensive to compute.\n",
    "ImportantWindowsVersions = (\n",
    "    '5.1.2',\n",
    "    '5.1.3',\n",
    "    '6.0.2',\n",
    "    '6.1.0',\n",
    "    '6.1.1',\n",
    "    '6.2.0',\n",
    "    '6.3.0',\n",
    "    '10.0.0',\n",
    ")\n",
    "\n",
    "def GetWindowsFeatures():\n",
    "    WindowsCompositorMap = WindowsFeatures.map(lambda p: (get_compositor(p),)).countByKey()\n",
    "    D3D11StatusMap = WindowsFeatures.map(lambda p: (get_d3d11_status(p),)).countByKey()\n",
    "    D2DStatusMap = WindowsFeatures.map(get_d2d_status).countByKey()\n",
    "    \n",
    "    warp_pings = WindowsFeatures.filter(lambda p: get_d3d11_status(p) == 'warp')\n",
    "    warp_pings = repartition(warp_pings)\n",
    "    WarpStatusMap = warp_pings.map(lambda p: (get_warp_status(p),)).countByKey()\n",
    "\n",
    "    TextureSharingMap = WindowsFeatures.filter(has_working_d3d11).map(get_texture_sharing_status).countByKey()\n",
    "    \n",
    "    blacklisted_pings = WindowsFeatures.filter(lambda p: get_d3d11_status(p) == 'blacklisted')\n",
    "    blacklisted_pings = repartition(blacklisted_pings)\n",
    "    blacklisted_devices = map_x_to_count(blacklisted_pings, 'deviceID')\n",
    "    blacklisted_drivers = map_x_to_count(blacklisted_pings, 'driverVersion')\n",
    "    blacklisted_os = map_x_to_count(blacklisted_pings, 'OSVersion')\n",
    "    blacklisted_pings = None \n",
    "    \n",
    "    blocked_pings = WindowsFeatures.filter(lambda p: get_d3d11_status(p) == 'blocked')\n",
    "    blocked_pings = repartition(blocked_pings)\n",
    "    blocked_vendors = map_x_to_count(blocked_pings, 'vendorID')\n",
    "    blocked_pings = None\n",
    "    \n",
    "    # Plugin models.\n",
    "    def aggregate_plugin_models(rdd):\n",
    "        rdd = rdd.filter(lambda p: p.get(PluginModelKey) is not None)\n",
    "        rdd = rdd.map(lambda p: p.get(PluginModelKey))\n",
    "        result = rdd.reduce(lambda x, y: x + y)\n",
    "        return [int(count) for count in result]\n",
    "    \n",
    "    plugin_models = aggregate_plugin_models(WindowsFeatures)\n",
    "    \n",
    "    # Media decoder backends.\n",
    "    def get_media_decoders(rdd):\n",
    "        rdd = rdd.filter(lambda p: p.get(MediaDecoderKey, None) is not None)\n",
    "        decoders = rdd.map(lambda p: p.get(MediaDecoderKey)).reduce(lambda x, y: x + y)\n",
    "        return [int(i) for i in decoders]\n",
    "    \n",
    "    media_decoders = get_media_decoders(WindowsFeatures)\n",
    "\n",
    "    # Now, build the same data except per version.\n",
    "    feature_pings_by_os = map_x_to_count(WindowsFeatures, 'OSVersion')\n",
    "    WindowsFeaturesByVersion = {}\n",
    "    for os_version in feature_pings_by_os:\n",
    "        if os_version not in ImportantWindowsVersions:\n",
    "            continue\n",
    "        subset = WindowsFeatures.filter(lambda p: p['OSVersion'] == os_version)\n",
    "        subset = repartition(subset)\n",
    "        \n",
    "        results = {\n",
    "            'count': subset.count(),\n",
    "            'compositors': subset.map(lambda p: (get_compositor(p),)).countByKey(),\n",
    "            'plugin_models': aggregate_plugin_models(subset),\n",
    "            'media_decoders': get_media_decoders(subset),\n",
    "        }\n",
    "        try:\n",
    "            if int(os_version.split('.')[0]) >= 6:\n",
    "                results['d3d11'] = subset.map(lambda p: (get_d3d11_status(p),)).countByKey()\n",
    "                results['d2d'] = subset.map(get_d2d_status).countByKey()\n",
    "                \n",
    "                warp_pings = subset.filter(lambda p: get_d3d11_status(p) == 'warp')\n",
    "                results['warp'] = warp_pings.map(lambda p: (get_warp_status(p),)).countByKey()\n",
    "        except:\n",
    "            pass\n",
    "        finally:\n",
    "            # Free resources.\n",
    "            warp_pings = None\n",
    "            subset = None\n",
    "        WindowsFeaturesByVersion[os_version] = results\n",
    "    \n",
    "    return {\n",
    "        'all': {\n",
    "            'compositors': WindowsCompositorMap,\n",
    "            'd3d11': D3D11StatusMap,\n",
    "            'd2d': D2DStatusMap,\n",
    "            'textureSharing': TextureSharingMap,\n",
    "            'warp': WarpStatusMap,\n",
    "            'plugin_models': plugin_models,\n",
    "            'media_decoders': media_decoders,\n",
    "        },\n",
    "        'byVersion': WindowsFeaturesByVersion,\n",
    "        'd3d11_blacklist': {\n",
    "            'devices': blacklisted_devices,\n",
    "            'drivers': blacklisted_drivers,\n",
    "            'os': blacklisted_os,\n",
    "        },\n",
    "        'd3d11_blocked': {\n",
    "            'vendors': blocked_vendors,\n",
    "        }\n",
    "    }\n",
    "\n",
    "TimedExport(filename = 'windows-features',\n",
    "            callback = GetWindowsFeatures,\n",
    "            pings = (WindowsFeatures, GeneralPingInfo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WindowsFeatures = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebGL Statistics\n",
    "_Note, this depends on running the \"Helpers for Compositor/Acceleration fields\" a few blocks above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetGLStatistics():\n",
    "    webgl_status_rdd = GeneralPings.filter(lambda p: p.get(WebGLFailureKey, None) is not None)\n",
    "    webgl_status_rdd = webgl_status_rdd.map(lambda p: p[WebGLFailureKey])\n",
    "    webgl_status_map = webgl_status_rdd.reduce(combiner)\n",
    "    webgl_accl_status_rdd = GeneralPings.filter(lambda p: p.get(WebGLAcclFailureKey, None) is not None)\n",
    "    webgl_accl_status_rdd = webgl_accl_status_rdd.map(lambda p: p[WebGLAcclFailureKey])\n",
    "    webgl_accl_status_map = webgl_accl_status_rdd.reduce(combiner)\n",
    "    return {\n",
    "        'webgl': {\n",
    "            'acceleration_status': webgl_accl_status_map,\n",
    "            'status': webgl_status_map,\n",
    "        }\n",
    "    }\n",
    "\n",
    "def WebGLStatisticsForKey(key):\n",
    "    histogram_pings = GeneralPings.filter(lambda p: p.get(key) is not None)\n",
    "    histogram_pings = repartition(histogram_pings)\n",
    "    \n",
    "    # Note - we're counting sessions where WebGL succeeded or failed,\n",
    "    # rather than the raw number of times either succeeded or failed.\n",
    "    # Also note that we don't double-count systems where both is true.\n",
    "    # Instead we only count a session's successes if it had no failures.\n",
    "    failure_rdd = histogram_pings.filter(lambda p: p[key][0] > 0)\n",
    "    success_rdd = histogram_pings.filter(lambda p: p[key][0] == 0 and p[key][1] > 0)\n",
    "    \n",
    "    failure_count = failure_rdd.count()\n",
    "    failure_by_os = map_x_to_count(failure_rdd, 'OS')\n",
    "    failure_by_vendor = map_x_to_count(failure_rdd, 'vendorID')\n",
    "    failure_by_device = map_x_to_count(failure_rdd, 'deviceID')\n",
    "    failure_by_driver = map_x_to_count(failure_rdd, 'driverVersion')\n",
    "    \n",
    "    success_count = success_rdd.count()\n",
    "    success_by_os = map_x_to_count(success_rdd, 'OS')\n",
    "    \n",
    "    def get_compositor_any_os(p):\n",
    "        if p['OSName'] != 'Windows':\n",
    "            # This data is not reliable yet - see bug 1247148.\n",
    "            return 'unknown'\n",
    "        return get_compositor(p)\n",
    "    success_by_cc = success_rdd.map(lambda p: (get_compositor_any_os(p),)).countByKey()\n",
    "\n",
    "    return {\n",
    "        'successes': {\n",
    "            'count': success_count,\n",
    "            'os': success_by_os,\n",
    "            'compositors': success_by_cc,\n",
    "        },\n",
    "        'failures': {\n",
    "            'count': failure_count,\n",
    "            'os': failure_by_os,\n",
    "            'vendors': failure_by_vendor,\n",
    "            'devices': failure_by_device,\n",
    "            'drivers': failure_by_driver,\n",
    "        },\n",
    "    }\n",
    "\n",
    "def GetWebGLStatistics():\n",
    "    return {\n",
    "        'webgl1': WebGLStatisticsForKey(WebGLSuccessKey),\n",
    "        'webgl2': WebGLStatisticsForKey(WebGL2SuccessKey),\n",
    "        'general': GetGLStatistics(),\n",
    "    }\n",
    "\n",
    "TimedExport(filename = 'webgl-statistics',\n",
    "            callback = GetWebGLStatistics,\n",
    "            pings = (GeneralPings, GeneralPingInfo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetLayersStatistics():\n",
    "    d3d9_status_rdd = GeneralPings.filter(lambda p: p.get(LayersD3D9FailureKey, None) is not None)\n",
    "    d3d9_status_rdd = d3d9_status_rdd.map(lambda p: p[LayersD3D9FailureKey])\n",
    "    d3d9_status_map = d3d9_status_rdd.reduce(combiner)\n",
    "    d3d11_status_rdd = GeneralPings.filter(lambda p: p.get(LayersD3D11FailureKey, None) is not None)\n",
    "    d3d11_status_rdd = d3d11_status_rdd.map(lambda p: p[LayersD3D11FailureKey])\n",
    "    d3d11_status_map = d3d11_status_rdd.reduce(combiner)\n",
    "    ogl_status_rdd = GeneralPings.filter(lambda p: p.get(LayersOGLFailureKey, None) is not None)\n",
    "    ogl_status_rdd = ogl_status_rdd.map(lambda p: p[LayersOGLFailureKey])\n",
    "    ogl_status_map = ogl_status_rdd.reduce(combiner)\n",
    "    return {\n",
    "        'layers': {\n",
    "            'd3d9': d3d9_status_map,\n",
    "            'd3d11': d3d11_status_map,\n",
    "        }\n",
    "    }\n",
    "\n",
    "def GetLayersStatistics():\n",
    "    return {\n",
    "        'general': GetLayersStatistics(),\n",
    "    }\n",
    "\n",
    "TimedExport(filename = 'layers-failureid-statistics',\n",
    "            callback = GetLayersStatistics,\n",
    "            pings = (GeneralPings, GeneralPingInfo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis End - Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Done with global pings.\n",
    "MacPings = None\n",
    "WindowsPings = None\n",
    "GeneralPings = None\n",
    "GeneralPingInfo = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EndTime = datetime.datetime.now()\n",
    "TotalElapsed = (EndTime - StartTime).total_seconds()\n",
    "\n",
    "print('Total time: {0}'.format(TotalElapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
